{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_Asif.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UDoUojD1IpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NOTE:\n",
        "### There are three different architecture used for WGAN in this notebook. Use the one you like.\n",
        "### WGAN can be used used to produce RGB images. But In this notebook, we used it to produce grayscale images.\n",
        "### Please provide path to folder/models where asked.\n",
        "### It is recommended to use colab for this notebook."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ3ANaPa3BoU",
        "colab_type": "text"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e04NPwoClfRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRW3cnvQ22pg",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCNYecZhMkKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### Initializing #######\n",
        "\n",
        "batch_size = 2\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 150\n",
        "#latent_dim = 150\n",
        "img_size = 64\n",
        "#img_size = 224\n",
        "channels = 1          ### It is recommended to keep it 1\n",
        "n_critic = 5\n",
        "clip_value = 0.01\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp5DQqaGo-Ka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "772fc058-c9f3-4c39-8414-67ebf36dd406"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwh5TBbG3KsQ",
        "colab_type": "text"
      },
      "source": [
        "# **WGAN Architectures Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAJ5AeK12Dd3",
        "colab_type": "text"
      },
      "source": [
        "## WGAN Architecture 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmZd-SqT2z_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsT9sRdT2FAu",
        "colab_type": "text"
      },
      "source": [
        "## WGAN Architecture 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TnarJ5120r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            *block(1024, 2048),\n",
        "            *block(2048, 4096),\n",
        "            nn.Linear(4096, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSSBVPTB18Hz",
        "colab_type": "text"
      },
      "source": [
        "## WGAN Architecture 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPCfPoHCICsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorG, self).__init__()\n",
        "\n",
        "        preprocess = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 2 * 2 * 768),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block1 = nn.Sequential( # 4\n",
        "            nn.ConvTranspose2d(768, 512, 2, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block2 = nn.Sequential( #16\n",
        "            nn.ConvTranspose2d(512, 256, 2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        block3 = nn.Sequential( #34\n",
        "            nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block4 = nn.Sequential( #70\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        deconv_out = nn.ConvTranspose2d(64, 1, 2, stride=2)\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "        self.block1 = block1\n",
        "        self.block2 = block2\n",
        "        self.block3 = block3\n",
        "        self.block4 = block4\n",
        "        self.deconv_out = deconv_out\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.preprocess(input)\n",
        "        output = output.view(-1, 768, 2, 2)\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.block3(output)\n",
        "        output = self.block4(output)\n",
        "        output = self.deconv_out(output)\n",
        "        output = self.tanh(output)\n",
        "        return output\n",
        "\n",
        "class DiscriminatorD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorD, self).__init__()\n",
        "        main = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 2, 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 2, 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 256, 2, 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(256, 512, 2, 2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        self.main = main\n",
        "        self.linear = nn.Linear(8192, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(-1, 8192)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oa5ikP_2JCa",
        "colab_type": "text"
      },
      "source": [
        "# Some Important functions definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpctu0NgM5ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apoctVYs2N1k",
        "colab_type": "text"
      },
      "source": [
        "# Loading generator and discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiMnvX0uMZzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "22e4fd96-4c5c-4815-b606-56ee5e4b4fdb"
      },
      "source": [
        "#################### Load Generator and Discriminator ####################\n",
        "generator = GeneratorG()\n",
        "discriminator = DiscriminatorD()\n",
        "\n",
        "#generator.load_state_dict(torch.load('*** Provide Path to saved Generator model ***'))\n",
        "#discriminator.load_state_dict(torch.load('*** Provide Path to saved Discriminator model ***'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjNsLe2V2bzZ",
        "colab_type": "text"
      },
      "source": [
        "# Loading dataset and setting up optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz4x-7rdMzkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss weight for gradient penalty\n",
        "lambda_gp = 10\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "data = datasets.ImageFolder('*** Provide Path to Target Datast ***', transform = transforms.Compose([transforms.ToTensor()]))\n",
        "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrI74_-N2q8K",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlh78TJm-kMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "714e36a4-6677-4db0-9613-41d484838299"
      },
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "start_time = time.time()\n",
        "sample_interval = 5000\n",
        "batches_done = 0\n",
        "os.makedirs(\"gen_images\", exist_ok=True)\n",
        "n_epochs = 2500\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "        imgs = imgs.sum(1)/3\n",
        "        imgs = imgs.view(imgs.shape[0],1,img_size,img_size)\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0],  latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z)\n",
        "\n",
        "        # Fake images\n",
        "        fake_validity = discriminator(fake_imgs)\n",
        "        # Real images\n",
        "        real_validity = discriminator(real_imgs)\n",
        "\n",
        "        # Gradient penalty\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Train the generator every n_critic steps\n",
        "        if i %  n_critic == 0:\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            # Generate a batch of images\n",
        "            fake_imgs = generator(z)\n",
        "            # Loss measures generator's ability to fool the discriminator\n",
        "            # Train on fake images\n",
        "            fake_validity = discriminator(fake_imgs)\n",
        "            g_loss = -torch.mean(fake_validity)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            if batches_done %  sample_interval == 0:\n",
        "                save_image(fake_imgs.data, \"gen_images/%d.png\" %(int(batches_done/sample_interval)), nrow=2, normalize=True)\n",
        "            batches_done +=  n_critic\n",
        "    \n",
        "    torch.save(generator.state_dict(), '*** Provide Path to save the Generator ***')\n",
        "    torch.save(discriminator.state_dict(), '*** Provide Path to save the Discriminator ***')\n",
        "    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch+1,  n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) )\n",
        "    print(\"\\n\\n--- %s Minutes ---\" % ((time.time() - start_time)/60))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 100/2500] [Batch 391/392] [D loss: 0.150654] [G loss: -208.621277]\n",
            "[Epoch 200/2500] [Batch 391/392] [D loss: -0.390665] [G loss: -544.014282]\n",
            "[Epoch 300/2500] [Batch 391/392] [D loss: 3.136023] [G loss: -5.197406]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}